{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9su91zgftHt",
    "outputId": "cf44117f-86b5-44f5-8c22-15229758c6ea"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "PATH_PREFIX = '/content/drive/My Drive/NLP-group-project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WTm8rMsef2Yn",
    "outputId": "050c9211-094b-414f-9bd6-8f01dc37a391"
   },
   "outputs": [],
   "source": [
    "!cd /content/drive/My\\ Drive/NLP-group-project && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bv06dSfkgBDm",
    "outputId": "7723724b-71e4-4d96-e42a-aa15ac07617e"
   },
   "outputs": [],
   "source": [
    "!cd /content/drive/My\\ Drive/NLP-group-project && sh setup_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iiezEIcU_vv"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elWJQ48Di2gZ"
   },
   "outputs": [],
   "source": [
    "class MassiveDatasetHerbert(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_path: str,\n",
    "        tokenizer: AutoTokenizer.from_pretrained('allegro/herbert-base-cased'),\n",
    "        labels_values_path = PATH_PREFIX + '/data/labels.json',\n",
    "    ):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._inputs = None\n",
    "        self._targets = None\n",
    "\n",
    "        with open(labels_values_path, 'r') as file:\n",
    "            self.labels_values = json.load(file)\n",
    "        self.idx_to_label = {i: label for i, label in enumerate(self.labels_values)}\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(self.labels_values)}\n",
    "\n",
    "        with open(json_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        self._encode(data)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        item = {\n",
    "          'input_ids': self._inputs['input_ids'][index],\n",
    "          'attention_mask': self._inputs['attention_mask'][index],\n",
    "          'labels': self._targets[index]\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def _one_hot_encode_labels(self, labels: list[str]) -> torch.Tensor:\n",
    "        encoded_labels = torch.zeros((len(labels), len(self.labels_values)))\n",
    "        label_indices = [self.label_to_idx[label] for label in labels]\n",
    "        encoded_labels[torch.arange(len(labels)), label_indices] = 1.0\n",
    "        return encoded_labels\n",
    "\n",
    "    def _decode_one_hot_labels(self, encoded_labels: torch.Tensor) -> list[str]:\n",
    "        \"\"\"\n",
    "        :param encoded_labels: Two dimensional tensor where each row should contain single\n",
    "        non zero value.\n",
    "        \"\"\"\n",
    "        labels = [self.idx_to_label[torch.argmax(enc_label)] for enc_label in encoded_labels]\n",
    "        return labels\n",
    "\n",
    "    def _encode(self, data: dict):\n",
    "        \"\"\"\n",
    "        Encode inputs with tokenizer and outputs into one-hot format.\n",
    "        \"\"\"\n",
    "        self._inputs = self._tokenizer(data['x'], padding='longest', return_tensors='pt')\n",
    "        self._targets = self._one_hot_encode_labels(data['y'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDF7WE6hVDfk"
   },
   "outputs": [],
   "source": [
    "with open(PATH_PREFIX + '/data/labels.json', 'r') as file:\n",
    "    labels = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLQpLumnVIEn"
   },
   "outputs": [],
   "source": [
    "NUM_LABELS = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cld_KvSJm8yz"
   },
   "outputs": [],
   "source": [
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir=PATH_PREFIX + \"/.out/herbert/\",\n",
    "    logging_dir=PATH_PREFIX + \"/.log/herbert/\",\n",
    "    logging_strategy='steps',\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=4.5e-5,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6lE5Ly6rCks"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  metric = evaluate.load('accuracy')\n",
    "  logits, labels = eval_pred\n",
    "\n",
    "  predictions = np.argmax(logits, axis=1)\n",
    "  labels = np.argmax(labels, axis=1)\n",
    "\n",
    "  return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herbert Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4yqMlPSPQkC"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allegro/herbert-base-cased')\n",
    "\n",
    "train_dataset_pl = MassiveDatasetHerbert(PATH_PREFIX + '/data/pl-PL/train.json', tokenizer)\n",
    "test_dataset_pl = MassiveDatasetHerbert(PATH_PREFIX + '/data/pl-PL/test.json', tokenizer)\n",
    "val_dataset_pl = MassiveDatasetHerbert(PATH_PREFIX + '/data/pl-PL/val.json', tokenizer)\n",
    "\n",
    "def new_environment(train_dataset, test_dataset):\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"allegro/herbert-base-cased\",\n",
    "    num_labels=NUM_LABELS\n",
    "  )\n",
    "\n",
    "  trainer = Trainer(\n",
    "      model=model,\n",
    "      args=TRAINING_ARGS,\n",
    "      train_dataset=train_dataset,\n",
    "      eval_dataset=test_dataset,\n",
    "      compute_metrics=compute_metrics\n",
    "  )\n",
    "\n",
    "  return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "YxygLeGTit6U",
    "outputId": "86b90aa2-bb27-4e35-e02c-5fa786035d46"
   },
   "outputs": [],
   "source": [
    "model, trainer = new_environment(train_dataset_pl, test_dataset_pl)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
